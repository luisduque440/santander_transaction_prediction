{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of pickled pipelines\n",
    "Notice that `feature_importance.ipynb` was used to generate the file `feature_importance.csv` which ranks the importance of the features according to several metrics. \n",
    "\n",
    "Our goal is to implement the following pipelines using all, top20, top50, top100 and top150 features\n",
    "* Pipeline 1: Logistic regression (`o`)\n",
    "* Pipeline 2: Naive bayes\n",
    "* Pipeline 3: Random forest\n",
    "* Pipeline 4: XGBoost\n",
    "* Pipeline 5: Neural Network [To do]\n",
    "* Pipeline 6: SVM [To do]\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from pipeline_utilities import create_base_pipeline\n",
    "from pipeline_utilities import pickle_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('train.csv').set_index('ID_code')\n",
    "#test_csv = pd.read_csv('test.csv').set_index('ID_code')\n",
    "train_csv = pd.read_csv('train.csv').set_index('ID_code')\n",
    "#test_csv = pd.read_csv('test.csv').set_index('ID_code')\n",
    "Features = train_csv.drop(columns = 'target')\n",
    "target = train_csv.target\n",
    "X, X_dropout, y, y_dropout = train_test_split(Features, target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 'most relevant' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_importance</th>\n",
       "      <th>linear_importance</th>\n",
       "      <th>linear_rank</th>\n",
       "      <th>tree_rank</th>\n",
       "      <th>average_rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>var_81</th>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.221647</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_139</th>\n",
       "      <td>0.011978</td>\n",
       "      <td>0.202066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_12</th>\n",
       "      <td>0.012252</td>\n",
       "      <td>0.187078</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_53</th>\n",
       "      <td>0.010661</td>\n",
       "      <td>0.177735</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_110</th>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.172094</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tree_importance  linear_importance  linear_rank  tree_rank  \\\n",
       "feature                                                               \n",
       "var_81          0.015771           0.221647          1.0        1.0   \n",
       "var_139         0.011978           0.202066          2.0        3.0   \n",
       "var_12          0.012252           0.187078          4.0        2.0   \n",
       "var_53          0.010661           0.177735          5.0        4.0   \n",
       "var_110         0.010287           0.172094          8.0        5.0   \n",
       "\n",
       "         average_rank  \n",
       "feature                \n",
       "var_81            1.0  \n",
       "var_139           2.5  \n",
       "var_12            3.0  \n",
       "var_53            4.5  \n",
       "var_110           6.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = pd.read_csv('feature_importance.csv', index_col='feature').sort_values(by='average_rank')\n",
    "top10_features = list(feature_importance.index[:10])\n",
    "top20_features = list(feature_importance.index[:20])\n",
    "top50_features = list(feature_importance.index[:50])\n",
    "top100_features = list(feature_importance.index[:100])\n",
    "top150_features = list(feature_importance.index[:150])\n",
    "all_features = list(feature_importance.index)\n",
    "feature_importance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the pipelines we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline_descriptors = []\n",
    "\n",
    "# linear classifier\n",
    "classifier = LogisticRegression(penalty='l1')\n",
    "parameters = {'C': 10**np.linspace(-4,-2, 20)}\n",
    "base_pipeline_descriptors.append((classifier, parameters, top10_features, 'linear_classifier_top10.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top20_features, 'linear_classifier_top20.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top50_features, 'linear_classifier_top50.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top100_features, 'linear_classifier_top100.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top150_features, 'linear_classifier_top150.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, all_features, 'linear_classifier_all.pkl'))\n",
    "\n",
    "\n",
    "\n",
    "# naive bayes\n",
    "classifier = GaussianNB()\n",
    "parameters = {}\n",
    "base_pipeline_descriptors.append((classifier, parameters, top10_features, 'naive_classifier_top10.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top20_features, 'naive_classifier_top20.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top50_features, 'naive_classifier_top50.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top100_features, 'naive_classifier_top100.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top150_features, 'naive_classifier_top150.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, all_features, 'naive_classifier_all.pkl'))\n",
    "\n",
    "\n",
    "# random forest\n",
    "classifier = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_leaf' : [10, 100, 1000],\n",
    "    'n_estimators': [50, 100, 200],   \n",
    "    'max_features': ['sqrt']\n",
    "    \n",
    "}\n",
    "base_pipeline_descriptors.append((classifier, parameters, top10_features, 'forest_classifier_top10.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top20_features, 'forest_classifier_top20.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top50_features, 'forest_classifier_top50.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top100_features, 'forest_classifier_top100.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top150_features, 'forest_classifier_top150.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, all_features, 'forest_classifier_all.pkl'))\n",
    "\n",
    "\n",
    "\n",
    "classifier = XGBClassifier()\n",
    "parameters = {\n",
    "    'max_depth':[5, 10, 20],   \n",
    "    'min_child_weight' : [10, 100, 1000], \n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'gamma': [0, 100]\n",
    "    \n",
    "}\n",
    "base_pipeline_descriptors.append((classifier, parameters, top10_features, 'xgboost_classifier_top10.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top20_features, 'xgboost_classifier_top20.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top50_features, 'xgboost_classifier_top50.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top100_features, 'xgboost_classifier_top100.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, top150_features, 'xgboost_classifier_top150.pkl'))\n",
    "base_pipeline_descriptors.append((classifier, parameters, all_features, 'xgboost_classifier_all.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation, training and pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forest_classifier_top20.pkl was created\n",
      "forest_classifier_top50.pkl was created\n"
     ]
    }
   ],
   "source": [
    "## main code\n",
    "for descriptor in base_pipeline_descriptors[13:18]:\n",
    "    classifier,parameters,features,filename = descriptor\n",
    "    base_pipeline = create_base_pipeline(descriptor, cv=2)\n",
    "    base_pipeline.fit(X,y)\n",
    "    pickle_pipeline(base_pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12646402, 0.08474043, 0.09292218, ..., 0.16774449, 0.13382349,\n",
       "       0.06882277])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_pipeline.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
